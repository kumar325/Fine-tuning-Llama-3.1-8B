cfg:
  mcore_gpt: true
  micro_batch_size: 1
  global_batch_size: 32
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  virtual_pipeline_model_parallel_size: null
  encoder_seq_length: 131072
  max_position_embeddings: 131072
  num_layers: 32
  hidden_size: 4096
  ffn_hidden_size: 14336
  num_attention_heads: 32
  init_method_std: 0.02
  use_scaled_init_method: true
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  kv_channels: null
  apply_query_key_layer_scaling: true
  normalization: rmsnorm
  layernorm_epsilon: 1.0e-05
  do_layer_norm_weight_decay: false
  make_vocab_size_divisible_by: 128
  pre_process: true
  post_process: true
  persist_layer_norm: true
  bias: false
  activation: fast-swiglu
  headscale: false
  transformer_block_type: pre_ln
  openai_gelu: false
  normalize_attention_scores: true
  position_embedding_type: rope
  rotary_percentage: 1.0
  attention_type: multihead
  share_embeddings_and_output_weights: false
  overlap_p2p_comm: false
  batch_p2p_comm: true
  num_query_groups: 8
  tokenizer:
    library: huggingface
    type: meta-llama/Meta-Llama-3-8B
    use_fast: true
  native_amp_init_scale: 4294967296
  native_amp_growth_interval: 1000
  hysteresis: 2
  fp32_residual_connection: false
  fp16_lm_cross_entropy: false
  megatron_amp_O2: true
  grad_allreduce_chunk_size_mb: 125
  grad_div_ar_fusion: true
  gradient_accumulation_fusion: false
  bias_activation_fusion: false
  bias_dropout_add_fusion: false
  masked_softmax_fusion: true
  get_attention_mask_from_fusion: true
  apply_rope_fusion: false
  seed: 1234
  resume_from_checkpoint: null
  use_cpu_initialization: false
  onnx_safe: false
  apex_transformer_log_level: 30
  gradient_as_bucket_view: false
  sync_batch_comm: false
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  num_micro_batches_with_partial_activation_checkpoints: null
  activations_checkpoint_layers_per_pipeline: null
  sequence_parallel: false
  transformer_engine: true
  fp8: false
  fp8_e4m3: false
  fp8_hybrid: true
  fp8_margin: 0
  fp8_interval: 1
  fp8_amax_history_len: 1024
  fp8_amax_compute_algo: max
  reduce_amax: true
  use_emha: false
  data:
    train_ds:
      file_names:
      - ./curated-data/law-qa-train_preprocessed.jsonl
      global_batch_size: 32
      micro_batch_size: 1
      shuffle: true
      num_workers: 0
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 2048
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      index_mapping_dir: null
      prompt_template: '{input} {output}'
      truncation_method: right
    validation_ds:
      file_names:
      - ./curated-data/law-qa-val_preprocessed.jsonl
      names: null
      global_batch_size: 32
      micro_batch_size: 1
      shuffle: false
      num_workers: 0
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 2048
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      index_mapping_dir: null
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
    test_ds:
      file_names: null
      names: null
      global_batch_size: 32
      micro_batch_size: 1
      shuffle: false
      num_workers: 0
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 2048
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      index_mapping_dir: null
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  nsys_profile:
    enabled: false
    start_step: 10
    end_step: 10
    ranks:
    - 0
    gen_shape: false
  optim:
    name: fused_adam
    lr: 0.0001
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 50
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  rotary_base: 500000.0
  scale_positional_embedding: true
  precision: bf16-mixed
  target: nemo.collections.nlp.models.language_modeling.megatron_gpt_model.MegatronGPTModel
  nemo_version: 2.0.0rc2
  restore_from_path: ./llama-3_1-8b-instruct-nemo_v1.0/llama3_1_8b_instruct.nemo
  save_nemo_on_validation_end: false
  answer_only_loss: true
  fsdp: false
  fsdp_sharding_strategy: full
  fsdp_grad_reduce_dtype: fp32
  fsdp_sharded_checkpoint: false
  fsdp_use_orig_params: false
  peft:
    peft_scheme: lora
    restore_from_path: null
    adapter_tuning:
      type: parallel_adapter
      adapter_dim: 32
      adapter_dropout: 0.0
      norm_position: pre
      column_init_method: xavier
      row_init_method: zero
      norm_type: mixedfusedlayernorm
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
    lora_tuning:
      variant: nemo
      target_modules:
      - attention_qkv
      adapter_dim: 32
      alpha: 32
      adapter_dropout: 0.0
      column_init_method: xavier
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
    p_tuning:
      virtual_tokens: 10
      bottleneck_dim: 1024
      embedding_dim: 1024
      init_std: 0.023
    ia3_tuning:
      layer_selection: null
    selective_tuning:
      tunable_base_param_names:
      - self_attention
      - word_embeddings
